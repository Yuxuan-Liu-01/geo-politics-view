import streamlit as st
import pandas as pd
import plotly.express as px
from collections import Counter
import re

# ================== é…ç½® ==================
CSV_FILE = "last10day_filtered.csv"
WEIBO_CSV_FILE = "å¾®åšçƒ­æœ_å—æµ·.xlsx"


COUNTRIES = {
    'ä¸­å›½', 'ç¾å›½', 'è²å¾‹å®¾', 'åŠ æ‹¿å¤§', 'è‹±å›½', 'æ—¥æœ¬', 'æ¾³å¤§åˆ©äºš', 'è¶Šå—', 'å°æ¹¾',
    'éŸ©å›½', 'ä¿„ç½—æ–¯', 'å°åº¦', 'æœé²œ', 'é©¬æ¥è¥¿äºš', 'å°å°¼', 'æ–°åŠ å¡', 'æ³•å›½', 'å¾·å›½',
    'ä¸œç›Ÿ', 'åŒ—çº¦', 'æ¬§ç›Ÿ', 'è”åˆå›½', 'è·å…°', 'æ–°è¥¿å…°', 'æ³°å›½', 'å·´è¥¿'
}

LOCATIONS = {
    # åŸæœ‰åœ°ç‚¹
    'å—æµ·', 'å°æµ·', 'é»„å²©å²›', 'ä»çˆ±ç¤', 'ç¾æµå²›', 'æ°¸æš‘ç¤', 'å·´å£«æµ·å³¡',
    'å¤ªå¹³æ´‹', 'ä¸œæµ·', 'å†²ç»³', 'å…³å²›', 'å¤å¨å¤·', 'ä¸œå—äºš',
    'è¥¿è²å¾‹å®¾æµ·', 'æ–¯å¡ä¼¯å‹’æµ…æ»©', 'å—æµ·äº‰è®®æµ…æ»©', 'å—æµ·æµ·åŸŸ','è²å¾‹å®¾','å—ä¸­å›½æµ·','ä¸œæµ·','æµ·æ–¯å¡ä¼¯å‹’æµ…æ»©','å°æ¹¾æµ·å³¡','å°å¤ªåœ°åŒº',
    # æ–°å¢åœ°ç‚¹
    'é©¬æ¥è¥¿äºš',
    'å‰éš†å¡',
    'é»„å²©å²›ç©ºåŸŸ',
    'é»„å²©å²›é™„è¿‘æ°´åŸŸ',
    'é»„æµ·',
    'é»ç‰™å®æ¯”å¸‚',
    # åˆ«å
    'Bajo de Masinloc',
    'Scarborough Shoal'
}

ORGANIZATIONS = {
    'å›½é˜²éƒ¨', 'äº”è§’å¤§æ¥¼', 'ç™½å®«', 'å›½åŠ¡é™¢', 'å¤–äº¤éƒ¨', 'ä¸­å›½æµ·è­¦', 'è§£æ”¾å†›',
    'CNN', 'BBC', 'æ–°åç¤¾', 'è·¯é€ç¤¾', 'å½­åšç¤¾', 'çº½çº¦æ—¶æŠ¥', 'åç››é¡¿é‚®æŠ¥',
    'ä¸œç›Ÿ', 'åŒ—çº¦', 'è”åˆå›½', 'é‡‘ç –å›½å®¶', 'NRDC','ç¾å›½å›½é˜²éƒ¨é•¿','è‹±å›½çš‡å®¶ç©ºå†›','æ¾³å¤§åˆ©äºšçš‡å®¶æµ·å†›','è²å¾‹å®¾é©»åç››é¡¿ç‰¹ä½¿',
    'ç¾å›½å«ç”Ÿä¸å…¬ä¼—æœåŠ¡éƒ¨','ä¸­å›½å…±äº§å…š','ç¾å›½æ€»ç»Ÿ','è²å¾‹å®¾æµ·å²¸è­¦å«é˜Ÿ','ä¸­å›½èˆ¹åª','åŠ æ‹¿å¤§çš‡å®¶æµ·å†›','ç¾å›½æµ·å†›',
    'AMTI', 'ä¸­å›½äººæ°‘è§£æ”¾å†›å—éƒ¨æˆ˜åŒº', 'ç¾å›½ç©ºå†›', 'è²å¾‹å®¾ç©ºå†›','è²å¾‹å®¾æ­¦è£…éƒ¨é˜Ÿ','è²å¾‹å®¾æ”¿åºœ','è²å¾‹å®¾æµ·å†›','ä¸­å›½æµ·å†›','æ¸”æ°‘',
    'æ—¥æœ¬æµ·ä¸Šè‡ªå«é˜Ÿ', 'è²å¾‹å®¾æµ·å†›', 'SmartNews', 'Daily Caller',
    'Glenn Beck', 'Manila Times'
}

EVENT_TYPE_KEYWORDS = {
    'è”åˆå·¡èˆª': [
        'å·¡èˆª', 'èˆªè¡Œè‡ªç”±', 'å†›èˆ°', 'èˆªæ¯', 'é©±é€èˆ°', 'èˆ°é˜Ÿ', 'æ¼”ä¹ ', 'å†›æ¼”', 'å†›äº‹æ¼”ä¹ ', 'åˆä½œ', 'æµ·è¯•',
        'éƒ¨ç½²', 'å¤šè¾¹', 'è”åˆ', 'B-1', 'è½°ç‚¸æœº', 'FA-50', 'æˆ˜æ–—æœº', 'é£è¡Œä½œä¸š', 'æµ·ä¸Šåˆä½œæ´»åŠ¨','è®­ç»ƒ','å·¡é€»'
    ],
    'è¡¥ç»™/å»ºè®¾': [
        'è¡¥ç»™', 'å»ºè®¾', 'å¡«æµ·', 'åŸºå»º', 'é©»å®ˆ', 'ç‰©èµ„', 'é©³èˆ¹', 'ç™»é™†æ¼”ä¹ ',
        'èƒ½æºå‹˜æ¢', 'å¼€å‘', 'åŒºå—', 'åœ°å›¾', 'åˆ©ç›Šç›¸å…³è€…','å‚¨å¤‡','çŸ³æ²¹','å¡«æµ·','å»ºé€ ','äººå·¥å²›','å¼€é‡‡'
    ],
    'æ‰§æ³•å¯¹å³™': [
        'æµ·è­¦', 'æ‰§æ³•', 'å¯¹å³™', 'æ‹¦æˆª', 'é©±ç¦»', 'ç™»èˆ¹', 'å†²çª', 'å¯¹æŠ—', 'æ”»å‡»', 'å†›äº‹è¡ŒåŠ¨',
        'ç¢°æ’', 'æ°´ç‚®', 'èˆ¹åªç¢°æ’', 'æŒ‘è¡…æ»‹äº‹', 'ä¾µç•¥','éæ³•å·¡é€»','éªšæ‰°äº‹ä»¶','ä¾¦å¯Ÿ','å¯¹æŠ—','å†²çª','ç›‘æµ‹',
        'ç›‘æ§','è¢­å‡»','æˆ˜äº‰','ä¾µç•¥','ä¾µçŠ¯','éœ¸å‡Œ'
    ],
    'å¤–äº¤å£°æ˜': [
        'å£°æ˜', 'æŠ—è®®', 'è°´è´£', 'è¡¨æ€', 'å¤–äº¤', 'ç…§ä¼š', 'è¨€è®º', 'äº¤é”‹', 'å¨èƒ', 'è­¦å‘Š',
        'éæ³•ä¸»å¼ ', 'è°è¨€', 'ä¸»æƒå£°ç´¢', 'é˜²åŠ¡æ‰¿è¯º', 'å¤–éƒ¨åŠ¿åŠ›', 'å¾’åŠ³æ— åŠŸ','åŠ å¼ºåˆä½œ','æ–°é—»å‘å¸ƒä¼š','ä¼šè°ˆ',
        'å¬è¯ä¼š','å¯»æ±‚å…±åŒç«‹åœº','é‡ç”³','ä¸»å¼ ','ä¸»æƒ','å€¡è®®'
    ],
    'èˆ†è®ºè§†é¢‘': [
        'è§†é¢‘', 'æ›å…‰', 'ç›´æ’­', 'ç¤¾äº¤åª’ä½“', 'Twitter', 'X.com', 'ç…§ç‰‡',
        'å‘å¸ƒåœ°å›¾', 'æ›´æ–°ä¿¡æ¯', 'æ–°é—»', 'æŠ¥é“', 'æ–‡ç« ', 'é“¾æ¥','äº‰è®®','è®¨è®º','å‘¼å','å»ºè®®','å›½é™…å‹åŠ›','æ‹…å¿§','ç‚’ä½œ'
    ]
}

# ================== æ•°æ®åŠ è½½ä¸æ¸…æ´— ==================

def extract_urls_from_row(row):
    urls = set()
    for col in row.index[7:]:
        cell = str(row[col]).strip()
        if not cell or cell == 'nan':
            continue
        parts = cell.replace('"', '').split(';')
        for part in parts:
            part = part.strip()
            if part.startswith('http'):
                urls.add(part)
    return list(urls)

def classify_event_type(text):
    text = str(text).lower()
    for event_type, keywords in EVENT_TYPE_KEYWORDS.items():
        if any(kw.lower() in text for kw in keywords):
            return event_type
    return 'å…¶ä»–'

@st.cache_data
def load_and_process_data():
    df = pd.read_csv(CSV_FILE, dtype=str, keep_default_na=False)
    
    base_cols = ['ä¸»äº‹ä»¶æ ‡é¢˜', 'äº‹ä»¶æ ‡é¢˜', 'æ—¶é—´', 'åœ°ç‚¹', 'æ¶‰äº‹æ–¹', 'å…³é”®åŠ¨ä½œ', 'æ€»æ¨æ–‡æ•°']
    if len(df.columns) >= 7:
        df = df.iloc[:, :107]
        df.columns = base_cols + [f'é“¾æ¥{i}' for i in range(1, len(df.columns)-6)]
    else:
        st.error("CSV åˆ—æ•°ä¸è¶³")
        st.stop()

    df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'], errors='coerce')
    df['æ€»æ¨æ–‡æ•°'] = pd.to_numeric(df['æ€»æ¨æ–‡æ•°'], errors='coerce').fillna(0).astype(int)
    df = df.dropna(subset=['æ—¶é—´']).sort_values('æ—¶é—´').reset_index(drop=True)

    df['è¯æ®é“¾æ¥åˆ—è¡¨'] = df.apply(extract_urls_from_row, axis=1)
    df['äº‹ä»¶ç±»å‹'] = df['äº‹ä»¶æ ‡é¢˜'].apply(classify_event_type)

    # === æå–å…ƒæ•°æ®ï¼šæŒ‰é¢‘æ¬¡æ’åº ===
    location_list = [
        loc.strip()
        for locs in df['åœ°ç‚¹'].str.split('ã€')
        for loc in locs
        if loc.strip()
    ]
    location_counts = Counter(location_list)
    all_locations = [item for item, count in location_counts.most_common()]

    party_list = [
        p.strip()
        for parties in df['æ¶‰äº‹æ–¹'].str.split('ã€')
        for p in parties
        if p.strip()
    ]
    party_counts = Counter(party_list)
    all_parties = [item for item, count in party_counts.most_common()]

    metadata = {
        'all_locations': all_locations,
        'all_parties': all_parties,
        'location_counts': location_counts,
        'party_counts': party_counts,
    }

    return df, metadata

# ================== åŠ è½½å¾®åšçƒ­æœæ•°æ® ==================
@st.cache_data
def load_weibo_hotsearch():
    try:
        weibo_df = pd.read_excel(WEIBO_CSV_FILE, sheet_name="Sheet1")
        weibo_df['date'] = pd.to_datetime(weibo_df['date']).dt.date  # è½¬ä¸º date ç±»å‹
        return weibo_df[['date', 'title']]
    except Exception as e:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ°å¾®åšçƒ­æœæ–‡ä»¶æˆ–è¯»å–å¤±è´¥ï¼š{e}")
        return pd.DataFrame(columns=['date', 'title'])

# ================== å®ä½“æå– ==================

def extract_entities_from_row(row):
    text = f"{row['äº‹ä»¶æ ‡é¢˜']} {row['å…³é”®åŠ¨ä½œ']} {row['æ¶‰äº‹æ–¹']} {row['åœ°ç‚¹']}"
    entities = {
        'å›½å®¶/åœ°åŒº': [e for e in COUNTRIES if e in text],
        'åœ°ç‚¹': [e for e in LOCATIONS if e in text],
        'ç»„ç»‡': [e for e in ORGANIZATIONS if e in text],
    }
    return entities

# ================== ä¸»åº”ç”¨ ==================

def main():
    st.set_page_config(page_title="åœ°ç¼˜æ”¿æ²»äº‹ä»¶èšç±»åˆ†æç³»ç»Ÿ", layout="wide")
    st.title("åœ°ç¼˜æ”¿æ²»äº‹ä»¶èšç±»åˆ†æç³»ç»Ÿ")

    try:
        df, metadata = load_and_process_data()
    except Exception as e:
        st.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
        st.stop()

    # ===== ä¾§è¾¹æ ç­›é€‰ =====
    st.sidebar.header("ğŸ” å…¨å±€ç­›é€‰")
    min_date = df['æ—¶é—´'].min().date()
    max_date = df['æ—¶é—´'].max().date()
    start_date, end_date = st.sidebar.date_input(
        "ğŸ“… æ—¶é—´èŒƒå›´",
        value=(min_date, max_date),
        min_value=min_date,
        max_value=max_date
    )

    selected_loc = st.sidebar.multiselect("åœ°ç‚¹", metadata['all_locations'])
    selected_party = st.sidebar.multiselect("æ¶‰äº‹æ–¹", metadata['all_parties'])

    # è¿‡æ»¤
    filtered_df = df[
        (df['æ—¶é—´'].dt.date >= start_date) &
        (df['æ—¶é—´'].dt.date <= end_date)
    ]
    if selected_loc:
        filtered_df = filtered_df[filtered_df['åœ°ç‚¹'].str.contains('|'.join(selected_loc), na=False)]
    if selected_party:
        filtered_df = filtered_df[filtered_df['æ¶‰äº‹æ–¹'].str.contains('|'.join(selected_party), na=False)]

    if filtered_df.empty:
        st.warning("âš ï¸ å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ— æ•°æ®")
        st.stop()

    # ===== åŠ è½½å¾®åšçƒ­æœæ•°æ® =====
    weibo_df = load_weibo_hotsearch()

    # ===== åŠŸèƒ½ 1ï¼šå¹´åº¦èŠ‚ç‚¹è¡¨ï¼ˆå«å¾®åšçƒ­æœå¯¹æ¯”ï¼‰=====
    st.header("å¹´åº¦èŠ‚ç‚¹è¡¨")
    window_size_days = 10
    filtered_df['window_start'] = filtered_df['æ—¶é—´'].dt.floor(f'{window_size_days}D')

    # è‡ªå®šä¹‰èšåˆå‡½æ•°ï¼šæŒ‰ä¸»äº‹ä»¶æ±‡æ€»æ¨æ–‡æ•°ï¼Œå– Top 3
    def aggregate_window(group):
        # æŒ‰ä¸»äº‹ä»¶æ ‡é¢˜èšåˆæ€»æ¨æ–‡æ•°
        main_event_reach = group.groupby('ä¸»äº‹ä»¶æ ‡é¢˜')['æ€»æ¨æ–‡æ•°'].sum().nlargest(3)
        top_events_str = 'ï¼›'.join(main_event_reach.index)
        return pd.Series({
            'çƒ­åº¦': group['æ€»æ¨æ–‡æ•°'].sum(),
            'äº‹ä»¶æ•°': len(group),
            'Topäº‹ä»¶': top_events_str
        })

    node_table = filtered_df.groupby('window_start').apply(aggregate_window).reset_index()

    node_table['çª—å£ç»“æŸ'] = node_table['window_start'] + pd.Timedelta(days=window_size_days - 1)
    node_table['window_start'] = node_table['window_start'].dt.strftime('%Y-%m-%d')
    node_table['çª—å£ç»“æŸ'] = node_table['çª—å£ç»“æŸ'].dt.strftime('%Y-%m-%d')
    node_table = node_table.sort_values('window_start', ascending=False)

    # æ–°å¢ï¼šåŒ¹é…å¾®åšçƒ­æœ
    def get_hotsearch_info(window_start_str):
        window_start = pd.to_datetime(window_start_str).date()
        window_end = window_start + pd.Timedelta(days=9)
        matched = weibo_df[
            (weibo_df['date'] >= window_start) &
            (weibo_df['date'] <= window_end)
        ]
        titles = matched['title'].tolist()
        count = len(titles)
        # æœ€å¤šæ˜¾ç¤º5æ¡ï¼Œé¿å…è¿‡é•¿
        display_titles = "ï¼›".join(titles[:5]) if titles else "æ— "
        return count, display_titles

    # åº”ç”¨å‡½æ•°
    hotsearch_info = node_table['window_start'].apply(get_hotsearch_info)
    node_table['çƒ­æœæ•°é‡'] = [x[0] for x in hotsearch_info]
    node_table['ç›¸å…³å¾®åšçƒ­æœ'] = [x[1] for x in hotsearch_info]

    # æ˜¾ç¤ºè¡¨æ ¼
    st.dataframe(
        node_table[[
            'window_start', 'çª—å£ç»“æŸ', 'çƒ­åº¦', 'äº‹ä»¶æ•°', 'Topäº‹ä»¶',
            'çƒ­æœæ•°é‡', 'ç›¸å…³å¾®åšçƒ­æœ'
        ]],
        use_container_width=True
    )

    # ===== åŠŸèƒ½ 1.5ï¼šäº‹ä»¶ç±»å‹æ—¶é—´è¶‹åŠ¿ï¼ˆç´¯ç§¯ï¼‰=====
    st.header("äº‹ä»¶ç±»å‹æ—¶é—´è¶‹åŠ¿ï¼ˆ10å¤©çª—å£ Â· ç´¯ç§¯ï¼‰")
    trend_df = filtered_df.copy()
    trend_df['window'] = pd.to_datetime(trend_df['æ—¶é—´']).dt.to_period('10D').dt.start_time
    type_trend = trend_df.groupby(['window', 'äº‹ä»¶ç±»å‹']).size().reset_index(name='äº‹ä»¶æ•°')
    type_trend = type_trend.sort_values('window')
    type_trend['ç´¯è®¡äº‹ä»¶æ•°'] = type_trend.groupby('äº‹ä»¶ç±»å‹')['äº‹ä»¶æ•°'].cumsum()

    fig_trend = px.line(
        type_trend,
        x='window',
        y='ç´¯è®¡äº‹ä»¶æ•°',
        color='äº‹ä»¶ç±»å‹',
        title="å„ç±»äº‹ä»¶æ•°é‡éšæ—¶é—´å˜åŒ–ï¼ˆ10å¤©çª—å£ Â· ç´¯ç§¯ï¼‰",
        markers=False
    )
    fig_trend.update_layout(
        xaxis_title="æ—¶é—´",
        yaxis_title="ç´¯è®¡äº‹ä»¶æ•°é‡",
        hovermode="x unified",
        yaxis=dict(tickformat=',d')
    )
    st.plotly_chart(fig_trend, use_container_width=True)

    # ===== åŠŸèƒ½ 2.5ï¼šä¸»äº‹ä»¶å½±å“åŠ›åˆ†å¸ƒ =====
    st.header("ä¸»äº‹ä»¶å½±å“åŠ›åˆ†å¸ƒï¼ˆæŒ‰é¦–æ¬¡å‡ºç°æ—¶é—´ï¼‰")
    main_event_summary = filtered_df.groupby('ä¸»äº‹ä»¶æ ‡é¢˜').agg(
        å­äº‹ä»¶æ•°=('äº‹ä»¶æ ‡é¢˜', 'count'),
        æ€»æ¨æ–‡æ•°=('æ€»æ¨æ–‡æ•°', 'sum'),
        é¦–æ¬¡å‡ºç°=('æ—¶é—´', 'min')
    ).reset_index()

    type_mode = filtered_df.groupby('ä¸»äº‹ä»¶æ ‡é¢˜')['äº‹ä»¶ç±»å‹'].agg(
        lambda x: Counter(x).most_common(1)[0][0] if len(x) > 0 else 'å…¶ä»–'
    ).reset_index()
    main_event_summary = main_event_summary.merge(type_mode, on='ä¸»äº‹ä»¶æ ‡é¢˜')
    main_event_summary['é¦–æ¬¡å‡ºç°'] = pd.to_datetime(main_event_summary['é¦–æ¬¡å‡ºç°'])

    fig_bubble = px.scatter(
        main_event_summary,
        x='é¦–æ¬¡å‡ºç°',
        y='æ€»æ¨æ–‡æ•°',
        size='æ€»æ¨æ–‡æ•°',
        color='äº‹ä»¶ç±»å‹',
        hover_name='ä¸»äº‹ä»¶æ ‡é¢˜',
        hover_data={
            'é¦–æ¬¡å‡ºç°': '|%Y-%m-%d',
            'å­äº‹ä»¶æ•°': True,
            'æ€»æ¨æ–‡æ•°': ':,'
        },
        title="ä¸»äº‹ä»¶å½±å“åŠ›åˆ†å¸ƒï¼ˆXè½´ = é¦–æ¬¡å‡ºç°æ—¥æœŸï¼‰",
        size_max=60
    )
    fig_bubble.update_layout(
        xaxis_title="é¦–æ¬¡å‡ºç°æ—¥æœŸ",
        yaxis_title="æ€»æ¨æ–‡æ•°",
        xaxis=dict(tickformat='%Y-%m-%d')
    )
    st.plotly_chart(fig_bubble, use_container_width=True)

    # ===== åŠŸèƒ½ 2ï¼šäº‹ä»¶å¡ç‰‡åº“ =====
    st.header("äº‹ä»¶å¡ç‰‡åº“ï¼ˆæŒ‰ä¸»äº‹ä»¶èšåˆï¼‰")
    grouped = filtered_df.groupby('ä¸»äº‹ä»¶æ ‡é¢˜', sort=False)
    for main_event, group in grouped:
        group_sorted = group.sort_values('æ—¶é—´')
        first_date = group_sorted['æ—¶é—´'].iloc[0].strftime('%Y-%m-%d')
        total_sub_reach = group['æ€»æ¨æ–‡æ•°'].sum()
        sub_count = len(group)

        with st.expander(f"ğŸ—“ï¸ {first_date} | {main_event} | æ€»æ¨æ–‡æ•°: {total_sub_reach:,} | {sub_count} æ¡å­äº‹ä»¶"):
            if sub_count > 5:
                st.caption(f"å…± {sub_count} æ¡å­äº‹ä»¶ï¼ŒæŒ‰æ—¶é—´å€’åºå±•ç¤º")
            for _, row in group[::-1].iterrows():
                st.markdown(f"#### ğŸ—“ï¸ {row['æ—¶é—´'].strftime('%Y-%m-%d')} | æ¨æ–‡æ•°: {row['æ€»æ¨æ–‡æ•°']:,}")
                col1, col2 = st.columns([2, 1])
                with col1:
                    st.markdown(f"**æ ‡é¢˜**ï¼š{row['äº‹ä»¶æ ‡é¢˜']}")
                    st.markdown(f"**æ‘˜è¦**ï¼š{row['å…³é”®åŠ¨ä½œ']}")
                    st.markdown(f"**åœ°ç‚¹**ï¼š{row['åœ°ç‚¹']}")
                    st.markdown(f"**æ¶‰äº‹æ–¹**ï¼š{row['æ¶‰äº‹æ–¹']}")
                    urls = row['è¯æ®é“¾æ¥åˆ—è¡¨']
                    if urls:
                        st.markdown("**ğŸ”— è¯æ®é“¾æ¥**ï¼š")
                        for url in urls[:5]:
                            st.markdown(f"- [{url}]({url})")
                        if len(urls) > 5:
                            st.caption(f"... è¿˜æœ‰ {len(urls)-5} ä¸ªé“¾æ¥")
                with col2:
                    ents = extract_entities_from_row(row)
                    st.markdown("**æ¶‰åŠå®ä½“**")
                    for cat, items in ents.items():
                        if items:
                            st.markdown(f"- **{cat}**ï¼š{', '.join(set(items))}")
                st.divider()

    # ===== åŠŸèƒ½ 3ï¼šå®ä½“æ¦œ =====
    st.header("é«˜é¢‘å®ä½“æ¦œ")
    entity_counter = Counter()
    for _, row in filtered_df.iterrows():
        ents = extract_entities_from_row(row)
        for cat, items in ents.items():
            entity_counter.update([(cat, item) for item in items])

    if entity_counter:
        top_entities = entity_counter.most_common(20)
        ent_df = pd.DataFrame(top_entities, columns=['(ç±»åˆ«, å®ä½“)', 'é¢‘æ¬¡'])
        ent_df[['ç±»åˆ«', 'å®ä½“']] = pd.DataFrame(ent_df['(ç±»åˆ«, å®ä½“)'].tolist(), index=ent_df.index)
        ent_df = ent_df[['ç±»åˆ«', 'å®ä½“', 'é¢‘æ¬¡']]

        for category in ['å›½å®¶/åœ°åŒº', 'åœ°ç‚¹', 'ç»„ç»‡']:
            cat_data = ent_df[ent_df['ç±»åˆ«'] == category].head(6)
            if not cat_data.empty:
                fig = px.bar(
                    cat_data, y='å®ä½“', x='é¢‘æ¬¡', orientation='h',
                    title=f"ğŸ”¥ é«˜é¢‘{category}",
                    height=300
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("æœªè¯†åˆ«åˆ°é¢„å®šä¹‰å®ä½“")

    # ===== åŠŸèƒ½ 4ï¼šå¯ä»‹å…¥çª—å£å»ºè®® =====
    st.header("å¯ä»‹å…¥çª—å£å»ºè®®ï¼ˆæŒ‰äº‹ä»¶ç±»å‹ï¼‰")
    type_summary = filtered_df.groupby('äº‹ä»¶ç±»å‹').agg(
        äº‹ä»¶æ•°=('äº‹ä»¶æ ‡é¢˜', 'count'),
        æ€»æ¨æ–‡æ•°=('æ€»æ¨æ–‡æ•°', 'sum')
    ).reset_index().sort_values('æ€»æ¨æ–‡æ•°', ascending=False)
    st.dataframe(type_summary, use_container_width=True)

    st.markdown("""
    **ç±»å‹è¯´æ˜**ï¼š
    - **è”åˆå·¡èˆª**ï¼šå†›èˆ°è¡ŒåŠ¨ã€èˆªè¡Œè‡ªç”±
    - **è¡¥ç»™/å»ºè®¾**ï¼šå²›ç¤å»ºè®¾ã€é©³èˆ¹ã€ç™»é™†æ¼”ä¹ 
    - **æ‰§æ³•å¯¹å³™**ï¼šæµ·è­¦é©±ç¦»ã€æµ·ä¸Šå†²çª
    - **å¤–äº¤å£°æ˜**ï¼šå®˜æ–¹è¡¨æ€ã€è¨€è¯­äº¤é”‹
    - **èˆ†è®ºè§†é¢‘**ï¼šç¤¾äº¤åª’ä½“è§†é¢‘/å›¾ç‰‡æ›å…‰
    """)

    # ===== è°ƒè¯•é¢æ¿ï¼šæ˜¾ç¤ºé¢‘æ¬¡ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰=====
    with st.expander("åœ°ç‚¹ä¸æ¶‰äº‹æ–¹é¢‘æ¬¡ç»Ÿè®¡"):
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("åœ°ç‚¹é¢‘æ¬¡ï¼ˆTop 20ï¼‰")
            st.write(dict(metadata['location_counts'].most_common(20)))
        with col2:
            st.subheader("æ¶‰äº‹æ–¹é¢‘æ¬¡ï¼ˆTop 20ï¼‰")
            st.write(dict(metadata['party_counts'].most_common(20)))

if __name__ == "__main__":
    main()

