import streamlit as st
import pandas as pd
import plotly.express as px
from collections import Counter
import re

# ================== é…ç½® ==================
CSV_FILE = "ç¾å›½çƒ­åº¦å‰100äº‹ä»¶_10å¤©èšç±».csv"

# æ‰©å±•å›½å®¶/åœ°åŒºè¯å…¸ï¼ˆè¦†ç›–ä½ çš„æ•°æ®ï¼‰
COUNTRIES = {
    'ä¸­å›½', 'ç¾å›½', 'è²å¾‹å®¾', 'åŠ æ‹¿å¤§', 'è‹±å›½', 'æ—¥æœ¬', 'æ¾³å¤§åˆ©äºš', 'è¶Šå—', 'å°æ¹¾',
    'éŸ©å›½', 'ä¿„ç½—æ–¯', 'å°åº¦', 'æœé²œ', 'é©¬æ¥è¥¿äºš', 'å°å°¼', 'æ–°åŠ å¡', 'æ³•å›½', 'å¾·å›½',
    'ä¸œç›Ÿ', 'åŒ—çº¦', 'æ¬§ç›Ÿ', 'è”åˆå›½', 'è·å…°', 'æ–°è¥¿å…°', 'æ³°å›½', 'å·´è¥¿'
}
LOCATIONS = {
    'å—æµ·', 'å°æµ·', 'é»„å²©å²›', 'ä»çˆ±ç¤', 'ç¾æµå²›', 'æ°¸æš‘ç¤', 'å·´å£«æµ·å³¡',
    'å¤ªå¹³æ´‹', 'ä¸œæµ·', 'å†²ç»³', 'å…³å²›', 'å¤å¨å¤·', 'ä¸œå—äºš'
}
ORGANIZATIONS = {
    'å›½é˜²éƒ¨', 'äº”è§’å¤§æ¥¼', 'ç™½å®«', 'å›½åŠ¡é™¢', 'å¤–äº¤éƒ¨', 'ä¸­å›½æµ·è­¦', 'è§£æ”¾å†›',
    'CNN', 'BBC', 'æ–°åç¤¾', 'è·¯é€ç¤¾', 'å½­åšç¤¾', 'çº½çº¦æ—¶æŠ¥', 'åç››é¡¿é‚®æŠ¥',
    'ä¸œç›Ÿ', 'åŒ—çº¦', 'è”åˆå›½', 'é‡‘ç –å›½å®¶', 'NRDC','ç¾å›½å›½é˜²éƒ¨é•¿','è‹±å›½çš‡å®¶ç©ºå†›','æ¾³å¤§åˆ©äºšçš‡å®¶æµ·å†›','è²å¾‹å®¾é©»åç››é¡¿ç‰¹ä½¿',
    'ç¾å›½å«ç”Ÿä¸å…¬ä¼—æœåŠ¡éƒ¨','ä¸­å›½å…±äº§å…š','ç¾å›½æ€»ç»Ÿ','è²å¾‹å®¾æµ·å²¸è­¦å«é˜Ÿ','ä¸­å›½èˆ¹åª','åŠ æ‹¿å¤§çš‡å®¶æµ·å†›','ç¾å›½æµ·å†›'
}
# PEOPLE = {
#     'æ‹œç™»', 'ç‰¹æœ—æ™®', 'å¸ƒæ—è‚¯', 'å¥¥æ–¯æ±€', 'æ²™åˆ©æ–‡', 'ç‹æ¯…', 'ç§¦åˆš', 'å°é©¬ç§‘æ–¯',
#     'å²¸ç”°æ–‡é›„', 'é˜¿å°”å·´å†…å¡', 'æ™®äº¬', 'æ³½è¿æ–¯åŸº', 'æ‹‰ç±³', 'Stefanie Spear'
# }

EVENT_TYPE_KEYWORDS = {
    'è”åˆå·¡èˆª': ['å·¡èˆª', 'èˆªè¡Œè‡ªç”±', 'å†›èˆ°', 'èˆªæ¯', 'é©±é€èˆ°', 'èˆ°é˜Ÿ','æ¼”ä¹ ','å†›æ¼”','å†›äº‹æ¼”ä¹ ','åˆä½œ','æµ·è¯•'],
    'è¡¥ç»™/å»ºè®¾': ['è¡¥ç»™', 'å»ºè®¾', 'å¡«æµ·', 'åŸºå»º', 'é©»å®ˆ', 'ç‰©èµ„', 'é©³èˆ¹', 'ç™»é™†æ¼”ä¹ '],
    'æ‰§æ³•å¯¹å³™': ['æµ·è­¦', 'æ‰§æ³•', 'å¯¹å³™', 'æ‹¦æˆª', 'é©±ç¦»', 'ç™»èˆ¹', 'å†²çª','å¯¹æŠ—','æ”»å‡»','å†›äº‹è¡ŒåŠ¨'],
    'å¤–äº¤å£°æ˜': ['å£°æ˜', 'æŠ—è®®', 'è°´è´£', 'è¡¨æ€', 'å¤–äº¤', 'ç…§ä¼š', 'è¨€è®º', 'äº¤é”‹','å¨èƒ','è­¦å‘Š'],
    'èˆ†è®ºè§†é¢‘': ['è§†é¢‘', 'æ›å…‰', 'ç›´æ’­', 'ç¤¾äº¤åª’ä½“', 'Twitter', 'X.com', 'ç…§ç‰‡']
}

# ================== æ•°æ®åŠ è½½ä¸æ¸…æ´— ==================

def extract_urls_from_row(row):
    """ä»ç¬¬7åˆ—å¼€å§‹çš„æ‰€æœ‰é“¾æ¥åˆ—ä¸­æå–å»é‡URL"""
    urls = set()
    for col in row.index[7:]:
        cell = str(row[col]).strip()
        if not cell or cell == 'nan':
            continue
        # å¤„ç† "url1";"url2" æ ¼å¼
        parts = cell.replace('"', '').split(';')
        for part in parts:
            part = part.strip()
            if part.startswith('http'):
                urls.add(part)
    return list(urls)

def classify_event_type(text):
    text = str(text).lower()
    for event_type, keywords in EVENT_TYPE_KEYWORDS.items():
        if any(kw.lower() in text for kw in keywords):
            return event_type
    return 'å…¶ä»–'

@st.cache_data
def load_and_process_data():
    # è¯»å– CSVï¼ˆè‡ªåŠ¨è·³è¿‡ç©ºåˆ—ï¼‰
    df = pd.read_csv(CSV_FILE, dtype=str, keep_default_na=False)
    
    # é‡å‘½ååˆ—ï¼ˆç¡®ä¿å‰7åˆ—æ­£ç¡®ï¼‰
    base_cols = ['ä¸»äº‹ä»¶æ ‡é¢˜', 'äº‹ä»¶æ ‡é¢˜', 'æ—¶é—´', 'åœ°ç‚¹', 'æ¶‰äº‹æ–¹', 'å…³é”®åŠ¨ä½œ', 'æ€»è§¦è¾¾é‡']
    if len(df.columns) >= 7:
        df = df.iloc[:, :107]  # æˆªæ–­åˆ°åˆç†é•¿åº¦ï¼ˆå«é“¾æ¥ï¼‰
        df.columns = base_cols + [f'é“¾æ¥{i}' for i in range(1, len(df.columns)-6)]
    else:
        st.error("CSV åˆ—æ•°ä¸è¶³")
        st.stop()

    # è§£ææ—¶é—´ & è§¦è¾¾é‡
    df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'], errors='coerce')
    df['æ€»è§¦è¾¾é‡'] = pd.to_numeric(df['æ€»è§¦è¾¾é‡'], errors='coerce').fillna(0).astype(int)
    df = df.dropna(subset=['æ—¶é—´']).sort_values('æ—¶é—´').reset_index(drop=True)

    # æå–æ‰€æœ‰è¯æ®é“¾æ¥
    df['è¯æ®é“¾æ¥åˆ—è¡¨'] = df.apply(extract_urls_from_row, axis=1)

    # æ·»åŠ äº‹ä»¶ç±»å‹
    df['äº‹ä»¶ç±»å‹'] = df['äº‹ä»¶æ ‡é¢˜'].apply(classify_event_type)

    return df

# ================== å®ä½“æå– ==================

def extract_entities_from_row(row):
    text = f"{row['äº‹ä»¶æ ‡é¢˜']} {row['å…³é”®åŠ¨ä½œ']} {row['æ¶‰äº‹æ–¹']} {row['åœ°ç‚¹']}"
    entities = {
        'å›½å®¶/åœ°åŒº': [e for e in COUNTRIES if e in text],
        'åœ°ç‚¹': [e for e in LOCATIONS if e in text],
        'ç»„ç»‡': [e for e in ORGANIZATIONS if e in text],
    }
    return entities

# ================== ä¸»åº”ç”¨ ==================

def main():
    st.set_page_config(page_title="åœ°ç¼˜æ”¿æ²»äº‹ä»¶èšç±»åˆ†æç³»ç»Ÿ", layout="wide")
    st.title("ğŸŒ åœ°ç¼˜æ”¿æ²»äº‹ä»¶èšç±»åˆ†æç³»ç»Ÿ")

    try:
        df = load_and_process_data()
    except Exception as e:
        st.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
        st.stop()

    # ===== ä¾§è¾¹æ ç­›é€‰ =====
    st.sidebar.header("ğŸ” å…¨å±€ç­›é€‰")
    min_date = df['æ—¶é—´'].min().date()
    max_date = df['æ—¶é—´'].max().date()
    start_date, end_date = st.sidebar.date_input(
        "ğŸ“… æ—¶é—´èŒƒå›´",
        value=(min_date, max_date),
        min_value=min_date,
        max_value=max_date
    )

    all_locations = sorted(set(loc.strip() for locs in df['åœ°ç‚¹'].str.split('ã€') for loc in locs if loc.strip()))
    selected_loc = st.sidebar.multiselect("ğŸ“ åœ°ç‚¹", all_locations)

    all_parties = sorted(set(p.strip() for parties in df['æ¶‰äº‹æ–¹'].str.split('ã€') for p in parties if p.strip()))
    selected_party = st.sidebar.multiselect("ğŸ‘¥ æ¶‰äº‹æ–¹", all_parties)

    # è¿‡æ»¤
    filtered_df = df[
        (df['æ—¶é—´'].dt.date >= start_date) &
        (df['æ—¶é—´'].dt.date <= end_date)
    ]
    if selected_loc:
        filtered_df = filtered_df[filtered_df['åœ°ç‚¹'].str.contains('|'.join(selected_loc), na=False)]
    if selected_party:
        filtered_df = filtered_df[filtered_df['æ¶‰äº‹æ–¹'].str.contains('|'.join(selected_party), na=False)]

    if filtered_df.empty:
        st.warning("âš ï¸ å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ— æ•°æ®")
        st.stop()

    total_reach = filtered_df['æ€»è§¦è¾¾é‡'].sum()
    st.sidebar.metric("ğŸ“Š æ€»è§¦è¾¾é‡", f"{total_reach:,}")
    # ===== åŠŸèƒ½ 1ï¼šå¹´åº¦èŠ‚ç‚¹è¡¨ï¼ˆæŒ‰10å¤©çª—å£èšåˆè§¦è¾¾é‡ï¼‰=====
    st.header("1ï¸âƒ£ å¹´åº¦èŠ‚ç‚¹è¡¨")
    window_size_days = 10
    filtered_df['window_start'] = filtered_df['æ—¶é—´'].dt.floor(f'{window_size_days}D')
    node_table = filtered_df.groupby('window_start').agg(
        çƒ­åº¦=('æ€»è§¦è¾¾é‡', 'sum'),
        äº‹ä»¶æ•°=('äº‹ä»¶æ ‡é¢˜', 'count'),
        Topäº‹ä»¶=('ä¸»äº‹ä»¶æ ‡é¢˜', lambda x: 'ï¼›'.join(sorted(set(x))[:3]))
    ).reset_index()
    node_table['çª—å£ç»“æŸ'] = node_table['window_start'] + pd.Timedelta(days=window_size_days - 1)

    # âœ… å…³é”®ä¿®æ”¹ï¼šå°†æ—¶é—´åˆ—æ ¼å¼åŒ–ä¸º YYYY-MM-DD å­—ç¬¦ä¸²ï¼ˆä¸å¸¦å°æ—¶ï¼‰
    node_table['window_start'] = node_table['window_start'].dt.strftime('%Y-%m-%d')
    node_table['çª—å£ç»“æŸ'] = node_table['çª—å£ç»“æŸ'].dt.strftime('%Y-%m-%d')

    node_table = node_table.sort_values('window_start', ascending=False)
    st.dataframe(node_table[['window_start', 'çª—å£ç»“æŸ', 'çƒ­åº¦', 'äº‹ä»¶æ•°', 'Topäº‹ä»¶']], use_container_width=True)
    
   
    # ===== åŠŸèƒ½ 1.5ï¼šäº‹ä»¶ç±»å‹æ—¶é—´è¶‹åŠ¿ï¼ˆç´¯ç§¯ï¼‰=====
    st.header("ğŸ“ˆ äº‹ä»¶ç±»å‹æ—¶é—´è¶‹åŠ¿ï¼ˆ10å¤©çª—å£ Â· ç´¯ç§¯ï¼‰")
    trend_df = filtered_df.copy()
    trend_df['window'] = pd.to_datetime(trend_df['æ—¶é—´']).dt.to_period('10D').dt.start_time
    type_trend = trend_df.groupby(['window', 'äº‹ä»¶ç±»å‹']).size().reset_index(name='äº‹ä»¶æ•°')
    type_trend = type_trend.sort_values('window')

    # è®¡ç®—æ¯ä¸ªäº‹ä»¶ç±»å‹çš„ç´¯ç§¯å’Œ
    type_trend['ç´¯è®¡äº‹ä»¶æ•°'] = type_trend.groupby('äº‹ä»¶ç±»å‹')['äº‹ä»¶æ•°'].cumsum()

    fig_trend = px.line(
        type_trend,
        x='window',
        y='ç´¯è®¡äº‹ä»¶æ•°',
        color='äº‹ä»¶ç±»å‹',
        title="å„ç±»äº‹ä»¶æ•°é‡éšæ—¶é—´å˜åŒ–ï¼ˆ10å¤©çª—å£ Â· ç´¯ç§¯ï¼‰",
        markers=True
    )
    fig_trend.update_layout(
        xaxis_title="æ—¶é—´",
        yaxis_title="ç´¯è®¡äº‹ä»¶æ•°é‡",
        hovermode="x unified",
        yaxis=dict(tickformat=',d')  # å¼ºåˆ¶ Y è½´ä¸ºæ•´æ•°ï¼ˆä¸å¸¦å°æ•°ï¼‰
    )
    st.plotly_chart(fig_trend, use_container_width=True)

    # ===== åŠŸèƒ½ 2.5ï¼šä¸»äº‹ä»¶å½±å“åŠ›åˆ†å¸ƒï¼ˆæŒ‰æ—¶é—´ï¼‰=====
    st.header("ğŸ“Š ä¸»äº‹ä»¶å½±å“åŠ›åˆ†å¸ƒï¼ˆæŒ‰é¦–æ¬¡å‡ºç°æ—¶é—´ï¼‰")
    main_event_summary = filtered_df.groupby('ä¸»äº‹ä»¶æ ‡é¢˜').agg(
        å­äº‹ä»¶æ•°=('äº‹ä»¶æ ‡é¢˜', 'count'),
        æ€»è§¦è¾¾é‡=('æ€»è§¦è¾¾é‡', 'sum'),
        é¦–æ¬¡å‡ºç°=('æ—¶é—´', 'min')
    ).reset_index()

    # åˆå¹¶äº‹ä»¶ç±»å‹ï¼ˆå–æœ€å¸¸è§çš„ï¼‰
    type_mode = filtered_df.groupby('ä¸»äº‹ä»¶æ ‡é¢˜')['äº‹ä»¶ç±»å‹'].agg(
        lambda x: Counter(x).most_common(1)[0][0] if len(x) > 0 else 'å…¶ä»–'
    ).reset_index()
    main_event_summary = main_event_summary.merge(type_mode, on='ä¸»äº‹ä»¶æ ‡é¢˜')

    # ç¡®ä¿â€œé¦–æ¬¡å‡ºç°â€æ˜¯ datetime ç±»å‹ï¼ˆä¾¿äº Plotly å¤„ç†ï¼‰
    main_event_summary['é¦–æ¬¡å‡ºç°'] = pd.to_datetime(main_event_summary['é¦–æ¬¡å‡ºç°'])

    fig_bubble = px.scatter(
        main_event_summary,
        x='é¦–æ¬¡å‡ºç°',               # â† æ”¹ä¸ºæ—¥æœŸ
        y='æ€»è§¦è¾¾é‡',
        size='æ€»è§¦è¾¾é‡',
        color='äº‹ä»¶ç±»å‹',
        hover_name='ä¸»äº‹ä»¶æ ‡é¢˜',
        hover_data={
            'é¦–æ¬¡å‡ºç°': '|%Y-%m-%d',  # â† å…³é”®ï¼šæ‚¬åœåªæ˜¾ç¤ºå¹´æœˆæ—¥
            'å­äº‹ä»¶æ•°': True,
            'æ€»è§¦è¾¾é‡': ':,'
        },
        title="ä¸»äº‹ä»¶å½±å“åŠ›åˆ†å¸ƒï¼ˆXè½´ = é¦–æ¬¡å‡ºç°æ—¥æœŸï¼‰",
        size_max=60
    )
    fig_bubble.update_layout(
        xaxis_title="é¦–æ¬¡å‡ºç°æ—¥æœŸ",
        yaxis_title="æ€»è§¦è¾¾é‡",
        xaxis=dict(tickformat='%Y-%m-%d')  # å¯é€‰ï¼šXè½´åˆ»åº¦ä¹Ÿæ˜¾ç¤ºä¸ºæ—¥æœŸ
    )
    st.plotly_chart(fig_bubble, use_container_width=True)


    # ===== åŠŸèƒ½ 2ï¼šäº‹ä»¶å¡ç‰‡åº“ï¼ˆä¸¤çº§ç»“æ„ï¼šä¸»äº‹ä»¶ â†’ å­äº‹ä»¶ï¼‰=====
    st.header("2ï¸âƒ£ äº‹ä»¶å¡ç‰‡åº“ï¼ˆæŒ‰ä¸»äº‹ä»¶èšåˆï¼‰")
    
    # æŒ‰ä¸»äº‹ä»¶åˆ†ç»„
    grouped = filtered_df.groupby('ä¸»äº‹ä»¶æ ‡é¢˜', sort=False)
    
    for main_event, group in grouped:
        # è®¡ç®—è¯¥ä¸»äº‹ä»¶çš„æ€»è§¦è¾¾é‡å’Œå­äº‹ä»¶æ•°
        total_sub_reach = group['æ€»è§¦è¾¾é‡'].sum()
        sub_count = len(group)
        
        # æŒ‰æ—¶é—´æ’åºå­äº‹ä»¶
        group = group.sort_values('æ—¶é—´')
        # å¯¹å½“å‰ä¸»äº‹ä»¶çš„å­äº‹ä»¶æŒ‰æ—¶é—´æ’åºï¼ˆå‡åºï¼šæœ€æ—©åœ¨å‰ï¼‰
        group_sorted = group.sort_values('æ—¶é—´')
        first_date = group_sorted['æ—¶é—´'].iloc[0].strftime('%Y-%m-%d')
        total_sub_reach = group['æ€»è§¦è¾¾é‡'].sum()
        sub_count = len(group)

        with st.expander(f"ğŸ—“ï¸ {first_date} | ğŸ“ {main_event} | ğŸ”¥ æ€»è§¦è¾¾é‡: {total_sub_reach:,} | ğŸ“Œ {sub_count} æ¡å­äº‹ä»¶"):
            # å¦‚æœå­äº‹ä»¶è¾ƒå¤šï¼Œå¯è€ƒè™‘åŠ ä¸ªæç¤º
            if sub_count > 5:
                st.caption(f"å…± {sub_count} æ¡å­äº‹ä»¶ï¼ŒæŒ‰æ—¶é—´å€’åºå±•ç¤º")
            
            # å€’åºå±•ç¤ºï¼ˆæœ€æ–°åœ¨ä¸Šï¼‰
            for _, row in group[::-1].iterrows():
                st.markdown(f"#### ğŸ—“ï¸ {row['æ—¶é—´'].strftime('%Y-%m-%d')} | ğŸ”¥ {row['æ€»è§¦è¾¾é‡']:,}")
                col1, col2 = st.columns([2, 1])
                with col1:
                    st.markdown(f"**æ ‡é¢˜**ï¼š{row['äº‹ä»¶æ ‡é¢˜']}")
                    st.markdown(f"**æ‘˜è¦**ï¼š{row['å…³é”®åŠ¨ä½œ']}")
                    st.markdown(f"**åœ°ç‚¹**ï¼š{row['åœ°ç‚¹']}")
                    st.markdown(f"**æ¶‰äº‹æ–¹**ï¼š{row['æ¶‰äº‹æ–¹']}")
                    # æ˜¾ç¤ºè¯æ®é“¾æ¥
                    urls = row['è¯æ®é“¾æ¥åˆ—è¡¨']
                    if urls:
                        st.markdown("**ğŸ”— è¯æ®é“¾æ¥**ï¼š")
                        for url in urls[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                            st.markdown(f"- [{url}]({url})")
                        if len(urls) > 5:
                            st.caption(f"... è¿˜æœ‰ {len(urls)-5} ä¸ªé“¾æ¥")
                with col2:
                    ents = extract_entities_from_row(row)
                    st.markdown("**æ¶‰åŠå®ä½“**")
                    for cat, items in ents.items():
                        if items:
                            st.markdown(f"- **{cat}**ï¼š{', '.join(set(items))}")
                st.divider()  # åˆ†éš”çº¿

    # ===== åŠŸèƒ½ 3ï¼šå®ä½“æ¦œ =====
    st.header("3ï¸âƒ£ é«˜é¢‘å®ä½“æ¦œ")
    entity_counter = Counter()
    for _, row in filtered_df.iterrows():
        ents = extract_entities_from_row(row)
        for cat, items in ents.items():
            entity_counter.update([(cat, item) for item in items])

    if entity_counter:
        top_entities = entity_counter.most_common(20)
        ent_df = pd.DataFrame(top_entities, columns=['(ç±»åˆ«, å®ä½“)', 'é¢‘æ¬¡'])
        ent_df[['ç±»åˆ«', 'å®ä½“']] = pd.DataFrame(ent_df['(ç±»åˆ«, å®ä½“)'].tolist(), index=ent_df.index)
        ent_df = ent_df[['ç±»åˆ«', 'å®ä½“', 'é¢‘æ¬¡']]

        for category in ['å›½å®¶/åœ°åŒº', 'åœ°ç‚¹', 'ç»„ç»‡']:  # æ³¨æ„ï¼šä½ å·²æ³¨é‡Šæ‰ PEOPLEï¼Œæ‰€ä»¥å»æ‰ 'äººç‰©'
            cat_data = ent_df[ent_df['ç±»åˆ«'] == category].head(6)
            if not cat_data.empty:
                fig = px.bar(
                    cat_data, y='å®ä½“', x='é¢‘æ¬¡', orientation='h',
                    title=f"ğŸ”¥ é«˜é¢‘{category}",
                    height=300
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("æœªè¯†åˆ«åˆ°é¢„å®šä¹‰å®ä½“")

    # ===== åŠŸèƒ½ 4ï¼šå¯ä»‹å…¥çª—å£å»ºè®® =====
    st.header("4ï¸âƒ£ å¯ä»‹å…¥çª—å£å»ºè®®ï¼ˆæŒ‰äº‹ä»¶ç±»å‹ï¼‰")
    type_summary = filtered_df.groupby('äº‹ä»¶ç±»å‹').agg(
        äº‹ä»¶æ•°=('äº‹ä»¶æ ‡é¢˜', 'count'),
        æ€»è§¦è¾¾é‡=('æ€»è§¦è¾¾é‡', 'sum')
    ).reset_index().sort_values('æ€»è§¦è¾¾é‡', ascending=False)
    st.dataframe(type_summary, use_container_width=True)

    st.markdown("""
    **ç±»å‹è¯´æ˜**ï¼š
    - **è”åˆå·¡èˆª**ï¼šå†›èˆ°è¡ŒåŠ¨ã€èˆªè¡Œè‡ªç”±
    - **è¡¥ç»™/å»ºè®¾**ï¼šå²›ç¤å»ºè®¾ã€é©³èˆ¹ã€ç™»é™†æ¼”ä¹ 
    - **æ‰§æ³•å¯¹å³™**ï¼šæµ·è­¦é©±ç¦»ã€æµ·ä¸Šå†²çª
    - **å¤–äº¤å£°æ˜**ï¼šå®˜æ–¹è¡¨æ€ã€è¨€è¯­äº¤é”‹
    - **èˆ†è®ºè§†é¢‘**ï¼šç¤¾äº¤åª’ä½“è§†é¢‘/å›¾ç‰‡æ›å…‰
    """)

if __name__ == "__main__":

    main()
